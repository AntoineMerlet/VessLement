{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Computer Aided Diagnosis: Diagnosis in dermoscopic images\n",
    "## Valerio Di Sano, Antoine Merlet\n",
    "\n",
    "![alt text](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQqOdINfzZ4LQu82vL_1PYgMLL8jISvCGTF5fY71zMr01weZ7gGdQ \"UdG\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer: This Notebook aims to demonstrate our results on Skin Lesion Classification using Deep Learning.\n",
    "### All data (Software, Model weights, Packages) should be installed and organized as stated at the end of this file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "best_acc = 0 # \"Global\" holding the best accuracy reached\n",
    "writer = SummaryWriter('runs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './'  # Path to the dataset\n",
    "nb_classes = 2    # Number of classes\n",
    "\n",
    "# Network\n",
    "arch = 'vgg16_bn' # Architecture of the CNN. Use 'vgg16_bn', 'ResNet50' or 'GoogLeNet'\n",
    "epochs = 100      # Number of epochs\n",
    "checkpoint = ''   # Load saved checkpoint\n",
    "start_epoch = 0   # Starting epoch in checkpoint\n",
    "batch_size = 16   # Number of data inputed into CNN at once. Change depending on GPU RAM\n",
    "\n",
    "# Optimizer setting\n",
    "optimizer = 'adam'    # Choice of the optimizer SGD or adam\n",
    "learning_rate = 1e-3 # Initial optimizer Learning Rate (LR)\n",
    "lr_decay_fact = 0.1  # Multiplier for learning rate reduction\n",
    "lr_decay_time = 7    # Number of epochs before learning_rate * lr_decay_fact\n",
    "momentum_SGD = 0.9   # Momentum for SGD\n",
    "weight_d = 1e-4      # Weight decay (L2 penalty)\n",
    "\n",
    "# Run type\n",
    "evaluate = False  # Set to True to skip training and evaluate only\n",
    "pretrained = True # set True to load Pytorch pretrained weights (ImageNet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tool functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- Compile without reading is fine ----------------------------\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Decrease the Learning Rate by multiplier on given number of epoch\"\"\"\n",
    "    lr = learning_rate * (lr_decay_fact ** (epoch // lr_decay_time))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "def accuracy(output, target, topk=(1,)): # TODOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(1, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'): # TODO export in folder with all param\n",
    "    torch.save(state, filename)\n",
    "    if is_best: # if new best model, export weights\n",
    "        shutil.copyfile(filename, 'best_model.pth.tar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL section to do. unuasable\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute gradient and do  step\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.set_grad_enabled(True):\n",
    "            # compute output\n",
    "            output = model(input_var)\n",
    "            loss = criterion(output, target_var)\n",
    "            \n",
    "            _, preds = torch.max(output, 1)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(1, 1))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        acc.update(prec1[0], input.size(0))\n",
    "\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Accuracy {acc.val:.3f} ({acc.avg:.3f})\\t'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, acc=acc))\n",
    "            niter = epoch*len(train_loader)+i\n",
    "            writer.add_scalar('Train/Loss', losses.val, niter)\n",
    "            writer.add_scalar('Train/Accuracy', acc.val, niter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL section to do. unuasable\n",
    "def validate(val_loader, model, criterion, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        target = target.cuda(async=True)\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            input_var = torch.autograd.Variable(input)\n",
    "            target_var = torch.autograd.Variable(target)\n",
    "    \n",
    "            # compute output\n",
    "            output = model(input_var)\n",
    "            loss = criterion(output, target_var)\n",
    "    \n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(output.data, target, topk=(1, 1))\n",
    "            losses.update(loss.data[0], input.size(0))\n",
    "            top1.update(prec1[0], input.size(0))\n",
    "    \n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "    \n",
    "            if i % 10 == 0:\n",
    "                print('Validation: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Accuracy {top1.val:.3f} ({top1.avg:.3f})\\t'.format(\n",
    "                       i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                       top1=top1))\n",
    "    print(' * Accuracy {top1.avg:.3f}'\n",
    "          .format(top1=top1))\n",
    "    writer.add_scalar('Validation/Loss', losses.avg, epoch)\n",
    "    writer.add_scalar('Validation/Accuracy', top1.avg, epoch)\n",
    "\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using VGG16_bn (batch normalization)\n"
     ]
    }
   ],
   "source": [
    "# Load architecture\n",
    "if arch == 'vgg16_bn':\n",
    "    print(\"Using VGG16_bn (batch normalization)\")\n",
    "    model = models.vgg16_bn(pretrained=True)    # Load given model with pretrained weigths (download if needed)\n",
    "    num_ftrs = model.classifier[6].in_features  # Get number of output of the second last layer\n",
    "    model.classifier[6] = nn.Linear(num_ftrs,nb_classes)   # Reset last layer weights, change number of output\n",
    "    model.features = torch.nn.DataParallel(model.features) # Needed for local processing\n",
    "    model.cuda() # Transfer model to GPU\n",
    "elif arch == 'ResNet50':\n",
    "    model = models.ResNet50(pretrained=True) # TODOOOOOOOOOOOO\n",
    "    torch.nn.DataParallel(model).cuda() # Transfer model to GPU\n",
    "elif arch == 'GoogLeNet':\n",
    "    model = models.GoogLeNet(pretrained=True) # TODOOOOOOOOOOOO\n",
    "    torch.nn.DataParallel(model).cuda() # Transfer model to GPU\n",
    "else :\n",
    "    print('Error: Unrecognized architecture. Exiting...') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training checkpoint\n",
    "if checkpoint: \n",
    "    if os.path.isfile(checkpoint):\n",
    "        data = torch.load(checkpoint)   # get the file\n",
    "        start_epoch = data['epoch']     # load previous epoch\n",
    "        best_acc = data['best_acc']     # load previous best accuracy\n",
    "        model.load_state_dict(data['state_dict'])    # load previous weights\n",
    "        optimizer.load_state_dict(data['optimizer']) # loadprevious optimiwzer stat\n",
    "        print(\"Loaded checkpoint '{}' (epoch {})\".format(checkpoint, data['epoch']))\n",
    "    else:\n",
    "        print(\"No checkpoint at '{}'\".format(checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define working tools\n",
    "if optimizer == 'SGD' :\n",
    "    optimizer = torch.optim.SGD(model.parameters(), learning_rate, momentum=momentum_SGD, weight_decay=weight_d)\n",
    "elif optimizer == 'adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), learning_rate, weight_decay=weight_d) # TODOOOOOOOOOOOOOOOOOOOO\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\Deep\\lib\\site-packages\\torchvision\\transforms\\transforms.py:563: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n",
      "  \"please use transforms.RandomResizedCrop instead.\")\n"
     ]
    }
   ],
   "source": [
    "train_dir = os.path.join(data_path, 'train') # Get train data folder\n",
    "\n",
    "train_dataset = datasets.ImageFolder( # Prepare training data\n",
    "    train_dir,\n",
    "    transforms.Compose([\n",
    "        transforms.RandomSizedCrop(224),   # randomly crop images to fit ImageNet input size\n",
    "        transforms.RandomHorizontalFlip(), # data augmentation\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize( # Setup normalization according to ImageNet\n",
    "            mean=[0.485, 0.456, 0.406], \n",
    "            std=[0.229, 0.224, 0.225]),\n",
    "    ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader( # Define loading schem\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, # Number of images per batch\n",
    "    shuffle=True,          # Shuffle data order on each epoch\n",
    "    pin_memory=True)       # Use CUDA pinned memory for tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\Deep\\lib\\site-packages\\torchvision\\transforms\\transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "val_dir = os.path.join(data_path, 'val') # Get validation data folder\n",
    "\n",
    "val_dataset = datasets.ImageFolder( # Prepare validation data\n",
    "    val_dir, \n",
    "    transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.CenterCrop(224), # Crop to fit input size. ROI assumed at center\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize( # Setup normalization according to ImageNet\n",
    "            mean=[0.485, 0.456, 0.406], \n",
    "            std=[0.229, 0.224, 0.225]),\n",
    "    ]))\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, # Explicitly reminded: do not shuffle for validation (for stats)\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\Deep\\lib\\site-packages\\ipykernel_launcher.py:39: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/300]\tTime 5.692 (5.692)\tData 0.184 (0.184)\tLoss 0.6181 (0.6181)\tAccuracy 68.750 (68.750)\t\n",
      "Epoch: [0][10/300]\tTime 0.261 (0.750)\tData 0.153 (0.167)\tLoss 0.6419 (2.7836)\tAccuracy 50.000 (56.250)\t\n",
      "Epoch: [0][20/300]\tTime 0.262 (0.523)\tData 0.180 (0.179)\tLoss 0.7800 (1.8640)\tAccuracy 56.250 (57.440)\t\n",
      "Epoch: [0][30/300]\tTime 0.269 (0.445)\tData 0.195 (0.187)\tLoss 0.4469 (1.4913)\tAccuracy 87.500 (57.258)\t\n",
      "Epoch: [0][40/300]\tTime 0.261 (0.403)\tData 0.180 (0.188)\tLoss 0.7621 (1.2743)\tAccuracy 50.000 (60.366)\t\n",
      "Epoch: [0][50/300]\tTime 0.268 (0.379)\tData 0.182 (0.191)\tLoss 0.5186 (1.1397)\tAccuracy 62.500 (60.417)\t\n"
     ]
    }
   ],
   "source": [
    "# --------- Evaluate only -------------\n",
    "if evaluate:\n",
    "    validate(val_loader, model, criterion, 1)\n",
    "# -------------------------------------\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    \n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "    accuracy = validate(val_loader, model, criterion, epoch)\n",
    "\n",
    "    is_best = accuracy > best_acc # is new accuracy global best\n",
    "    best_acc = max(accuracy, best_acc) # update best accuracy if needed\n",
    "    \n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': arch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Software and packages:\n",
    "* Python 3.6\n",
    "* CUDA 9.0\n",
    "* cuDNN 7.4.1\n",
    "\n",
    "* Pytorch 0.4.0\n",
    "* Torchvision \n",
    "\n",
    "#### Model weigths:\n",
    "If not given with this Notebook, please download them (1.10GB) here: https://drive.google.com/open?id=14mzlsTjZf4p-ihovbOTjyG_r1dZ2-alb   (not uploaded yet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
